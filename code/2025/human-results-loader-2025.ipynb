{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the xlsx sheets for without judgements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNFILES = {\n",
    "    \"auggen\": set([\n",
    "    \"arrange-final\",\n",
    "    \"cook-desperate\",\n",
    "    \"figure-civilian\",\n",
    "    \"keep-select\",\n",
    "    \"relevant-testimony\",\n",
    "    \"store-ethics\",\n",
    "    \"viewer-whisper\",\n",
    "    \"bite-stir\",\n",
    "    \"criteria-african\",\n",
    "    \"gently-disagree\",\n",
    "    \"neither-fitness\",\n",
    "    \"review-basic\",\n",
    "    \"that-darkness\",\n",
    "    \"both-rate\",\n",
    "    \"editor-elsewhere\",\n",
    "    \"image-climate\",\n",
    "    \"peer-necessarily\",\n",
    "    \"sake-frame\",\n",
    "    \"transportation-error\",\n",
    "    \"closer-submit\",\n",
    "    \"entrance-population\",\n",
    "    \"internal-withdraw\",\n",
    "    \"radical-twice\",\n",
    "    \"shock-description\",\n",
    "    \"vary-occasion\",\n",
    "]),\n",
    "\n",
    "\"gen\": set([\n",
    "\"activity-manufacturer\",\n",
    "\"bath-begin\",\n",
    "\"degree-mental\",\n",
    "\"flag-square\",\n",
    "\"listen-promise\",\n",
    "\"satellite-gear\",\n",
    "\"truth-muslim\",\n",
    "\"anger-interpretation\",\n",
    "\"battery-founder\",\n",
    "\"despite-deer\",\n",
    "\"flight-enhance\",\n",
    "\"mixture-pause\",\n",
    "\"scheme-presidential\",\n",
    "\"usual-disaster\",\n",
    "\"angle-apartment\",\n",
    "\"best-disaster\",\n",
    "\"dirty-shop\",\n",
    "\"hear-arrangement\",\n",
    "\"passenger-snow\",\n",
    "\"shortly-seek\",\n",
    "\"worry-repeat\",\n",
    "\"arab-exciting\",\n",
    "\"captain-wage\",\n",
    "\"distinction-standard\",\n",
    "\"image-hall\",\n",
    "\"past-fifth\",\n",
    "\"sport-broad\",\n",
    "\"area-goodbye\",\n",
    "\"child-poison\",\n",
    "\"distinguish-fitness\",\n",
    "\"institution-painting\",\n",
    "\"plant-indeed\",\n",
    "\"spread-retirement\",\n",
    "\"around-figure\",\n",
    "\"commercial-childhood\",\n",
    "\"eight-existence\",\n",
    "\"jacket-responsibility\",\n",
    "\"previously-accept\",\n",
    "\"status-produce\",\n",
    "\"attach-over\",\n",
    "\"concert-matter\",\n",
    "\"elect-lamp\",\n",
    "\"know-author\",\n",
    "\"protest-suspect\",\n",
    "\"temperature-thought\",\n",
    "\"badly-salary\",\n",
    "\"contemporary-dispute\",\n",
    "\"existing-engineering\",\n",
    "\"laboratory-movie\",\n",
    "\"pure-lawyer\",\n",
    "\"trend-shoot\"\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total anonymized run IDs: 76\n"
     ]
    }
   ],
   "source": [
    "orig_to_anon_map = {}\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/tags.auggen\", \"r\") as f:\n",
    "    auggen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for auggen_run_tag in auggen_run_tags:\n",
    "        orig_run_id = auggen_run_tag.split(\" \")[-1].strip()\n",
    "        anon_run_id = auggen_run_tag.split(\" \")[1].strip()\n",
    "        orig_to_anon_map[orig_run_id] = anon_run_id\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/tags.gen\", \"r\") as f:\n",
    "    gen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for gen_run_tag in gen_run_tags:\n",
    "        orig_run_id = gen_run_tag.split(\" \")[-1].strip()\n",
    "        anon_run_id = gen_run_tag.split(\" \")[1].strip()\n",
    "        orig_to_anon_map[orig_run_id] = anon_run_id\n",
    "\n",
    "print(f\"Total anonymized run IDs: {len(orig_to_anon_map)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total anonymized run IDs: 67\n"
     ]
    }
   ],
   "source": [
    "run_id_to_orig_map = {}\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/auggen-filename-run_id\", \"r\") as f:\n",
    "    auggen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for auggen_run_tag in auggen_run_tags:\n",
    "        run_id = auggen_run_tag.split(\" \")[-1].strip()\n",
    "        filenmae = auggen_run_tag.split(\" \")[0].strip()\n",
    "        if run_id in run_id_to_orig_map:\n",
    "            run_id_to_orig_map[run_id].append(filenmae)\n",
    "        else:\n",
    "            run_id_to_orig_map[run_id] = [filenmae]\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/gen-filename-run_id\", \"r\") as f:\n",
    "    gen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for gen_run_tag in gen_run_tags:\n",
    "        run_id = gen_run_tag.split(\" \")[-1].strip()\n",
    "        filenmae = gen_run_tag.split(\" \")[0].strip()\n",
    "        if run_id in run_id_to_orig_map:\n",
    "            run_id_to_orig_map[run_id].append(filenmae)\n",
    "        else:\n",
    "            run_id_to_orig_map[run_id] = [filenmae]\n",
    "\n",
    "print(f\"Total anonymized run IDs: {len(run_id_to_orig_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NITA_AG_JH', 'ag-run-1-JH']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_id_to_orig_map[\"ag-run-1-JH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mixture-pause', 'transportation-error']\n"
     ]
    }
   ],
   "source": [
    "def get_anonymous_run_id(run_id, orig_to_anon_map, run_id_to_orig_map):\n",
    "    \"\"\"Get the anonymous run ID for a given run ID.\"\"\"\n",
    "\n",
    "    all_ids = []\n",
    "\n",
    "    if run_id in RUNFILES[\"auggen\"] or run_id in RUNFILES[\"gen\"]:\n",
    "        all_ids.append(run_id)\n",
    "    \n",
    "    if run_id in orig_to_anon_map:\n",
    "        anon_run_id = orig_to_anon_map[run_id]\n",
    "        all_ids.append(anon_run_id)\n",
    "        \n",
    "    if run_id in run_id_to_orig_map:\n",
    "        original_run_id_list = run_id_to_orig_map[run_id]\n",
    "\n",
    "        for original_run_id in original_run_id_list:\n",
    "            anon_run_id = orig_to_anon_map.get(original_run_id, None)\n",
    "            if anon_run_id is not None:\n",
    "                all_ids.append(anon_run_id)\n",
    "    \n",
    "    return list(set(all_ids))\n",
    "\n",
    "print(get_anonymous_run_id(\"ag-run-1-JH\", orig_to_anon_map, run_id_to_orig_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_runfile(runfile_path):\n",
    "    runfile = {}\n",
    "    with open(runfile_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = json.loads(line)\n",
    "            topic_id = str(line[\"metadata\"][\"narrative_id\"])\n",
    "            if topic_id not in runfile:\n",
    "                if \"answer\" in line:\n",
    "                    runfile[topic_id] = {\"answer\": line[\"answer\"], \"references\": line.get(\"references\", [])}\n",
    "                elif \"responses\" in line:\n",
    "                    runfile[topic_id] = {\"answer\": line[\"responses\"], \"references\": line.get(\"references\", [])}\n",
    "                else:\n",
    "                    print(f\"Topic ID: {topic_id} has no answer or responses\")\n",
    "    return runfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure_out_which_anonymous_run_id(\n",
    "    anonymous_run_ids: list[str],\n",
    "    topic_id: str,\n",
    "    sentence: str,\n",
    "    sentence_id: int,\n",
    ") -> str:\n",
    "    \"\"\"Figure out which anonymous run ID to use for a given run ID.\"\"\"\n",
    "    RUNFILE_DIR = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/\"\n",
    "    \n",
    "    for anonymous_run_id in anonymous_run_ids:\n",
    "        # load the results file for the anonymous run ID\n",
    "        task = \"auggen\" if anonymous_run_id in RUNFILES[\"auggen\"] else \"gen\"\n",
    "        results = load_runfile(os.path.join(RUNFILE_DIR, f\"{task}/{anonymous_run_id}\"))\n",
    "        try:\n",
    "            sentence_anonymous = results[topic_id][\"answer\"][sentence_id]['text']\n",
    "            sentence_anonymous = sentence_anonymous.replace('\\r', '').replace('\\n', '').strip()\n",
    "            sentence = sentence.replace('\\r', '').replace('\\n', '').strip()\n",
    "\n",
    "            if sentence == sentence_anonymous:\n",
    "                return anonymous_run_id\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: 14_ I_m interested in sports_ societal impact, particularly concerning athlete compensation, inclusion,_.xlsx\n",
      "File: 72_ I want to understand why deforestation is such a major problem. Specifically, how does it impact the.xlsx\n",
      "File: 93_ I_m interested in learning how the U.S. Constitution and the Bill of Rights safeguard individual civ.xlsx\n",
      "File: 144_ I_m seeking a comprehensive understanding of financial institutions, specifically why banks like Sil.xlsx\n",
      "File: 161_ I want to understand the main arguments surrounding abortion and why people hold such different view.xlsx\n",
      "File: 200_ I want to deeply understand the Holocaust_ what it was, why and how it transpired, who was responsib.xlsx\n",
      "File: 213_ I_m looking into the Korean War to learn about its origins, how it ended, and why the US got involve.xlsx\n",
      "File: 224_ I want to understand why people immigrate or become refugees, the challenges they face, and how laws.xlsx\n",
      "File: 233_ I_m interested in learning how social media affects mental health, particularly among teenagers. I w.xlsx\n",
      "File: 273_ I want to understand why Africa, despite its rich resources, is often seen as underdeveloped or poor.xlsx\n",
      "File: 300_ I_m interested in learning about effective strategies to prevent and reduce global warming and clima.xlsx\n",
      "File: 407_ I_m curious why housing and rent prices have soared, particularly in places like Dubai, and what bro.xlsx\n",
      "File: 477_ I_m trying to understand the concept of race_ how it_s defined, debated as a social construct, and v.xlsx\n",
      "File: 499_ I_m hoping to understand the arguments for and against legalizing euthanasia, including how it diffe.xlsx\n",
      "File: 515_ I_m interested in learning why cancer rates are rising, particularly among young people, and whether.xlsx\n",
      "File: 707_ I_m trying to understand the health risks and potential dangers of various chemicals and substances,.xlsx\n",
      "File: 897_ I_m trying to understand how alcohol use affects neighborhood quality of life, including genetic and.xlsx\n"
     ]
    }
   ],
   "source": [
    "without_prediction_dir = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/rag_assessment/completed\"\n",
    "\n",
    "topicwise_results = {}\n",
    "# read all excel sheets one by one from the directory\n",
    "for file in os.listdir(without_prediction_dir):\n",
    "    complete_filepath = os.path.join(without_prediction_dir, file)\n",
    "    topic_id = str(file.split(\"_\")[0].strip())\n",
    "    topicwise_results[topic_id] = {}\n",
    "    \n",
    "    if complete_filepath.endswith(\".xlsx\"):\n",
    "        # Index(['CITED PASSAGE', 'SENTENCE', 'FULL', 'PARTIAL', 'NONE', 'SENTENCE CONTEXT', 'COMPLETED?', 'TASK', 'RUNID', 'DOCID', 'ANSWERID'], dtype='object')\n",
    "        print(f\"File: {file}\")\n",
    "        \n",
    "        df = pd.read_excel(complete_filepath)\n",
    "        for index, row in df.iterrows():\n",
    "            run_id = row[\"RUNID\"]\n",
    "            sentence = row[\"SENTENCE\"] if not pd.isna(row[\"SENTENCE\"]) else \"\"\n",
    "            sentence_id = row['ANSWERID']\n",
    "            doc_id = row['DOCID']\n",
    "\n",
    "            support_score = \"-1\"\n",
    "            # check if its under full, partial or none\n",
    "            # check if any or the rows is not NAN\n",
    "            if row['FULL'] in [\"x\", \"X\"]:\n",
    "                support_score = '2'\n",
    "            elif row['PARTIAL'] in [\"x\", \"X\"]:\n",
    "                support_score = '1'\n",
    "            elif row['NONE'] in [\"x\", \"X\"]:\n",
    "                support_score = '0'\n",
    "            \n",
    "            if run_id == \"grilllab-agentic-gpt4_1-5-larf-v2\":\n",
    "                run_id = \"grilllab-gpt45-gen\"\n",
    "\n",
    "            anonymous_run_ids = get_anonymous_run_id(run_id, orig_to_anon_map, run_id_to_orig_map)\n",
    "            anonymous_run_id = None\n",
    "\n",
    "            if len(anonymous_run_ids) == 0:\n",
    "                print(f\"No anonymous run ID found for run ID: {run_id}\")\n",
    "                continue\n",
    "\n",
    "            elif len(anonymous_run_ids) > 1:\n",
    "                anonymous_run_id = figure_out_which_anonymous_run_id(anonymous_run_ids, topic_id, sentence, sentence_id)\n",
    "            \n",
    "            elif len(anonymous_run_ids) == 1:\n",
    "                anonymous_run_id = anonymous_run_ids[0]\n",
    "\n",
    "            if anonymous_run_id is None:\n",
    "                print(f\"No anonymous run ID found for run ID: {run_id}\")\n",
    "                continue\n",
    "\n",
    "            if anonymous_run_id not in topicwise_results[topic_id]:\n",
    "\n",
    "                topicwise_results[topic_id][anonymous_run_id] = {\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"run_id\": anonymous_run_id,\n",
    "                    \"sentences\": [{\n",
    "                            \"sentenceID\": sentence_id,\n",
    "                            \"text\": sentence,\n",
    "                            \"citations\": [{\n",
    "                                'citationID': 0,\n",
    "                                'reference': doc_id,\n",
    "                                'support': support_score,\n",
    "                            }],\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            \n",
    "            # already the sentence has been defined\n",
    "            elif anonymous_run_id in topicwise_results[topic_id]: \n",
    "                topicwise_results[topic_id][anonymous_run_id][\"sentences\"].append(\n",
    "                    {\n",
    "                        \"sentenceID\": sentence_id,\n",
    "                        \"text\": sentence,\n",
    "                        \"citations\": [\n",
    "                            {\n",
    "                                'citationID': 0,\n",
    "                                'reference': doc_id,\n",
    "                                'support': support_score,\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing runs for topic  14 :  1\n",
      "Missing runs for topic  72 :  2\n",
      "Missing runs for topic  93 :  3\n",
      "Missing runs for topic  144 :  24\n",
      "Missing runs for topic  161 :  25\n",
      "Missing runs for topic  200 :  26\n",
      "Missing runs for topic  213 :  24\n",
      "Missing runs for topic  224 :  26\n",
      "Missing runs for topic  233 :  25\n",
      "Missing runs for topic  273 :  1\n",
      "Missing runs for topic  300 :  25\n",
      "Missing runs for topic  407 :  1\n",
      "Missing runs for topic  477 :  25\n",
      "Missing runs for topic  499 :  23\n",
      "Missing runs for topic  515 :  23\n",
      "Missing runs for topic  707 :  23\n",
      "Missing runs for topic  897 :  23\n"
     ]
    }
   ],
   "source": [
    "ALL_RUN_IDS = list(RUNFILES[\"auggen\"]) + list(RUNFILES[\"gen\"])\n",
    "\n",
    "all_unique_missing_runs = {}\n",
    "\n",
    "for topic_id in topicwise_results:\n",
    "    run_ids = set(topicwise_results[topic_id].keys())\n",
    "    # print(\"Missing runs for topic \", topic_id, \": \", len(set(ALL_RUN_IDS) - run_ids), \" --- \", set(ALL_RUN_IDS) - run_ids)\n",
    "    print(\"Missing runs for topic \", topic_id, \": \", len(set(ALL_RUN_IDS) - run_ids))\n",
    "    for missing_run in set(ALL_RUN_IDS) - run_ids:\n",
    "        if missing_run not in all_unique_missing_runs:\n",
    "            all_unique_missing_runs[missing_run] = 0\n",
    "        all_unique_missing_runs[missing_run] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_unique_missing_runs_sorted = sorted(all_unique_missing_runs, key=all_unique_missing_runs.get, reverse=True)\n",
    "# anon_to_orig_map = {v: k for k, v in orig_to_anon_map.items()}\n",
    "\n",
    "# for run in all_unique_missing_runs_sorted:\n",
    "#     original_run_id = anon_to_orig_map.get(run, \"UNKNOWN\")\n",
    "#     print(run, original_run_id, all_unique_missing_runs[run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic ID: 14\tRun ID: usual-disaster\tempty citation sentences: [0]\n",
      "topic ID: 14\tRun ID: internal-withdraw\tempty citation sentences: [1, 5]\n",
      "topic ID: 14\tRun ID: vary-occasion\tempty citation sentences: [6]\n",
      "topic ID: 14\tRun ID: peer-necessarily\tempty citation sentences: [0, 4, 6, 7]\n",
      "topic ID: 14\tRun ID: around-figure\tempty citation sentences: [0]\n",
      "topic ID: 14\tRun ID: review-basic\tempty citation sentences: [0]\n",
      "topic ID: 14\tRun ID: existing-engineering\tempty citation sentences: [0, 10]\n",
      "topic ID: 14\tRun ID: plant-indeed\tempty citation sentences: [0, 5]\n",
      "topic ID: 14\tRun ID: eight-existence\tempty citation sentences: [0]\n",
      "topic ID: 14\tRun ID: despite-deer\tempty citation sentences: [6]\n",
      "topic ID: 14\tRun ID: concert-matter\tempty citation sentences: [13]\n",
      "topic ID: 14\tRun ID: flag-square\tempty citation sentences: [0, 3, 4, 6]\n",
      "topic ID: 14\tRun ID: flight-enhance\tempty citation sentences: [0, 4]\n",
      "topic ID: 14\tRun ID: gently-disagree\tempty citation sentences: [0, 3, 4, 6]\n",
      "topic ID: 14\tRun ID: arrange-final\tempty citation sentences: [0, 4]\n",
      "topic ID: 14\tRun ID: temperature-thought\tempty citation sentences: [3, 4, 5]\n",
      "topic ID: 14\tRun ID: satellite-gear\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: usual-disaster\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: flight-enhance\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: jacket-responsibility\tempty citation sentences: [21]\n",
      "topic ID: 72\tRun ID: internal-withdraw\tempty citation sentences: [7]\n",
      "topic ID: 72\tRun ID: arrange-final\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: peer-necessarily\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: arab-exciting\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: concert-matter\tempty citation sentences: [0, 15]\n",
      "topic ID: 72\tRun ID: eight-existence\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: satellite-gear\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: flag-square\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: gently-disagree\tempty citation sentences: [0]\n",
      "topic ID: 72\tRun ID: temperature-thought\tempty citation sentences: [6, 7, 8]\n",
      "topic ID: 72\tRun ID: plant-indeed\tempty citation sentences: [0, 2, 3]\n",
      "topic ID: 93\tRun ID: flight-enhance\tempty citation sentences: [0, 1, 2, 11, 15, 18]\n",
      "topic ID: 93\tRun ID: arrange-final\tempty citation sentences: [0, 1, 2, 11, 15, 18]\n",
      "topic ID: 93\tRun ID: flag-square\tempty citation sentences: [0, 1, 10, 14]\n",
      "topic ID: 93\tRun ID: gently-disagree\tempty citation sentences: [0, 1, 10, 14]\n",
      "topic ID: 93\tRun ID: dirty-shop\tempty citation sentences: [0, 1, 3, 4, 6, 10]\n",
      "topic ID: 93\tRun ID: satellite-gear\tempty citation sentences: [0, 1, 4, 7, 8, 9, 13]\n",
      "topic ID: 93\tRun ID: arab-exciting\tempty citation sentences: [0]\n",
      "topic ID: 93\tRun ID: internal-withdraw\tempty citation sentences: [0, 1, 3]\n",
      "topic ID: 93\tRun ID: peer-necessarily\tempty citation sentences: [0, 1, 3, 5, 8, 9, 10, 12, 14, 15, 18]\n",
      "topic ID: 93\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 2, 5]\n",
      "topic ID: 93\tRun ID: plant-indeed\tempty citation sentences: [0, 5]\n",
      "topic ID: 144\tRun ID: dirty-shop\tempty citation sentences: [2, 5, 7]\n",
      "topic ID: 144\tRun ID: despite-deer\tempty citation sentences: [2, 5]\n",
      "topic ID: 144\tRun ID: satellite-gear\tempty citation sentences: [0, 3, 10]\n",
      "topic ID: 144\tRun ID: best-disaster\tempty citation sentences: [17, 18]\n",
      "topic ID: 144\tRun ID: flag-square\tempty citation sentences: [0, 4, 6, 12, 13]\n",
      "topic ID: 144\tRun ID: sport-broad\tempty citation sentences: [15]\n",
      "topic ID: 144\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 144\tRun ID: arrange-final\tempty citation sentences: [0, 5]\n",
      "topic ID: 144\tRun ID: vary-occasion\tempty citation sentences: [7]\n",
      "topic ID: 144\tRun ID: peer-necessarily\tempty citation sentences: [0, 1, 11, 12]\n",
      "topic ID: 144\tRun ID: plant-indeed\tempty citation sentences: [3, 4, 5, 7, 8]\n",
      "topic ID: 144\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "topic ID: 161\tRun ID: dirty-shop\tempty citation sentences: [5]\n",
      "topic ID: 161\tRun ID: image-hall\tempty citation sentences: [0]\n",
      "topic ID: 161\tRun ID: internal-withdraw\tempty citation sentences: [4]\n",
      "topic ID: 161\tRun ID: arrange-final\tempty citation sentences: [0, 5]\n",
      "topic ID: 161\tRun ID: flag-square\tempty citation sentences: [0, 4, 8]\n",
      "topic ID: 161\tRun ID: concert-matter\tempty citation sentences: [0, 13]\n",
      "topic ID: 161\tRun ID: badly-salary\tempty citation sentences: [16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33]\n",
      "topic ID: 161\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 2, 3, 7, 8, 9]\n",
      "topic ID: 161\tRun ID: plant-indeed\tempty citation sentences: [0, 1, 4, 5, 6, 8]\n",
      "topic ID: 161\tRun ID: sport-broad\tempty citation sentences: [13]\n",
      "topic ID: 161\tRun ID: listen-promise\tempty citation sentences: [14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "topic ID: 200\tRun ID: dirty-shop\tempty citation sentences: [5]\n",
      "topic ID: 200\tRun ID: internal-withdraw\tempty citation sentences: [10]\n",
      "topic ID: 200\tRun ID: flag-square\tempty citation sentences: [11, 14]\n",
      "topic ID: 200\tRun ID: previously-accept\tempty citation sentences: [9]\n",
      "topic ID: 200\tRun ID: plant-indeed\tempty citation sentences: [1, 4, 6]\n",
      "topic ID: 200\tRun ID: temperature-thought\tempty citation sentences: [6, 7, 8]\n",
      "topic ID: 200\tRun ID: jacket-responsibility\tempty citation sentences: [19]\n",
      "topic ID: 200\tRun ID: badly-salary\tempty citation sentences: [17, 18, 19, 20, 21, 22, 23, 24]\n",
      "topic ID: 200\tRun ID: worry-repeat\tempty citation sentences: [1]\n",
      "topic ID: 213\tRun ID: flag-square\tempty citation sentences: [0]\n",
      "topic ID: 213\tRun ID: sport-broad\tempty citation sentences: [17]\n",
      "topic ID: 213\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 213\tRun ID: badly-salary\tempty citation sentences: [17, 18, 19]\n",
      "topic ID: 213\tRun ID: internal-withdraw\tempty citation sentences: [2, 5, 7, 9, 11, 13]\n",
      "topic ID: 213\tRun ID: peer-necessarily\tempty citation sentences: [0, 5]\n",
      "topic ID: 213\tRun ID: review-basic\tempty citation sentences: [5]\n",
      "topic ID: 213\tRun ID: distinction-standard\tempty citation sentences: [0]\n",
      "topic ID: 213\tRun ID: image-hall\tempty citation sentences: [3]\n",
      "topic ID: 213\tRun ID: best-disaster\tempty citation sentences: [14]\n",
      "topic ID: 213\tRun ID: existing-engineering\tempty citation sentences: [0, 9]\n",
      "topic ID: 213\tRun ID: eight-existence\tempty citation sentences: [6]\n",
      "topic ID: 213\tRun ID: plant-indeed\tempty citation sentences: [0, 2, 5, 7]\n",
      "topic ID: 213\tRun ID: temperature-thought\tempty citation sentences: [0, 2, 5, 7]\n",
      "topic ID: 224\tRun ID: dirty-shop\tempty citation sentences: [8]\n",
      "topic ID: 224\tRun ID: flag-square\tempty citation sentences: [0, 4, 8, 12, 15]\n",
      "topic ID: 224\tRun ID: arrange-final\tempty citation sentences: [0, 5, 12]\n",
      "topic ID: 224\tRun ID: sport-broad\tempty citation sentences: [13]\n",
      "topic ID: 224\tRun ID: captain-wage\tempty citation sentences: [0]\n",
      "topic ID: 224\tRun ID: plant-indeed\tempty citation sentences: [3, 4]\n",
      "topic ID: 224\tRun ID: peer-necessarily\tempty citation sentences: [2]\n",
      "topic ID: 224\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 4, 5]\n",
      "topic ID: 224\tRun ID: badly-salary\tempty citation sentences: [20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "topic ID: 224\tRun ID: jacket-responsibility\tempty citation sentences: [14]\n",
      "topic ID: 233\tRun ID: dirty-shop\tempty citation sentences: [0]\n",
      "topic ID: 233\tRun ID: satellite-gear\tempty citation sentences: [0, 1, 6, 7]\n",
      "topic ID: 233\tRun ID: around-figure\tempty citation sentences: [0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\n",
      "topic ID: 233\tRun ID: internal-withdraw\tempty citation sentences: [0]\n",
      "topic ID: 233\tRun ID: peer-necessarily\tempty citation sentences: [0, 10, 11]\n",
      "topic ID: 233\tRun ID: battery-founder\tempty citation sentences: [5]\n",
      "topic ID: 233\tRun ID: arrange-final\tempty citation sentences: [0, 15]\n",
      "topic ID: 233\tRun ID: flag-square\tempty citation sentences: [0, 3, 11]\n",
      "topic ID: 233\tRun ID: temperature-thought\tempty citation sentences: [0, 2, 5]\n",
      "topic ID: 233\tRun ID: review-basic\tempty citation sentences: [0, 4, 7]\n",
      "topic ID: 233\tRun ID: captain-wage\tempty citation sentences: [0, 12]\n",
      "topic ID: 233\tRun ID: usual-disaster\tempty citation sentences: [0, 4]\n",
      "topic ID: 233\tRun ID: eight-existence\tempty citation sentences: [0, 4]\n",
      "topic ID: 233\tRun ID: plant-indeed\tempty citation sentences: [1]\n",
      "topic ID: 233\tRun ID: sport-broad\tempty citation sentences: [0]\n",
      "topic ID: 233\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 233\tRun ID: distinction-standard\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: battery-founder\tempty citation sentences: [9]\n",
      "topic ID: 273\tRun ID: eight-existence\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: dirty-shop\tempty citation sentences: [0, 10]\n",
      "topic ID: 273\tRun ID: despite-deer\tempty citation sentences: [9]\n",
      "topic ID: 273\tRun ID: peer-necessarily\tempty citation sentences: [0, 7]\n",
      "topic ID: 273\tRun ID: internal-withdraw\tempty citation sentences: [0, 7]\n",
      "topic ID: 273\tRun ID: vary-occasion\tempty citation sentences: [9]\n",
      "topic ID: 273\tRun ID: usual-disaster\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: review-basic\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: sport-broad\tempty citation sentences: [12]\n",
      "topic ID: 273\tRun ID: child-poison\tempty citation sentences: [4]\n",
      "topic ID: 273\tRun ID: previously-accept\tempty citation sentences: [9]\n",
      "topic ID: 273\tRun ID: hear-arrangement\tempty citation sentences: [1]\n",
      "topic ID: 273\tRun ID: satellite-gear\tempty citation sentences: [0, 6, 10]\n",
      "topic ID: 273\tRun ID: flag-square\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: flight-enhance\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: gently-disagree\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: arrange-final\tempty citation sentences: [0]\n",
      "topic ID: 273\tRun ID: temperature-thought\tempty citation sentences: [0, 5, 7]\n",
      "topic ID: 273\tRun ID: plant-indeed\tempty citation sentences: [0, 5, 7]\n",
      "topic ID: 300\tRun ID: satellite-gear\tempty citation sentences: [8]\n",
      "topic ID: 300\tRun ID: temperature-thought\tempty citation sentences: [0, 2, 7]\n",
      "topic ID: 300\tRun ID: plant-indeed\tempty citation sentences: [0, 1, 2, 4, 6]\n",
      "topic ID: 300\tRun ID: usual-disaster\tempty citation sentences: [12]\n",
      "topic ID: 300\tRun ID: arrange-final\tempty citation sentences: [4, 7, 9, 11]\n",
      "topic ID: 300\tRun ID: review-basic\tempty citation sentences: [0]\n",
      "topic ID: 300\tRun ID: eight-existence\tempty citation sentences: [7]\n",
      "topic ID: 300\tRun ID: peer-necessarily\tempty citation sentences: [0, 5]\n",
      "topic ID: 300\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 300\tRun ID: badly-salary\tempty citation sentences: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "topic ID: 407\tRun ID: satellite-gear\tempty citation sentences: [0, 3, 5, 6, 7, 12]\n",
      "topic ID: 407\tRun ID: flag-square\tempty citation sentences: [0, 5, 13]\n",
      "topic ID: 407\tRun ID: flight-enhance\tempty citation sentences: [0, 11]\n",
      "topic ID: 407\tRun ID: gently-disagree\tempty citation sentences: [0, 5, 13]\n",
      "topic ID: 407\tRun ID: arrange-final\tempty citation sentences: [0, 11]\n",
      "topic ID: 407\tRun ID: vary-occasion\tempty citation sentences: [1, 4, 5, 6, 7]\n",
      "topic ID: 407\tRun ID: previously-accept\tempty citation sentences: [23]\n",
      "topic ID: 407\tRun ID: concert-matter\tempty citation sentences: [12]\n",
      "topic ID: 407\tRun ID: child-poison\tempty citation sentences: [15, 17, 18]\n",
      "topic ID: 407\tRun ID: peer-necessarily\tempty citation sentences: [0, 2, 3, 8]\n",
      "topic ID: 407\tRun ID: jacket-responsibility\tempty citation sentences: [3, 9, 12]\n",
      "topic ID: 407\tRun ID: dirty-shop\tempty citation sentences: [2, 3, 5]\n",
      "topic ID: 407\tRun ID: despite-deer\tempty citation sentences: [2, 5, 6, 7, 8]\n",
      "topic ID: 407\tRun ID: temperature-thought\tempty citation sentences: [0, 5]\n",
      "topic ID: 407\tRun ID: eight-existence\tempty citation sentences: [0, 6]\n",
      "topic ID: 407\tRun ID: plant-indeed\tempty citation sentences: [0, 5]\n",
      "topic ID: 407\tRun ID: internal-withdraw\tempty citation sentences: [3, 4]\n",
      "topic ID: 477\tRun ID: dirty-shop\tempty citation sentences: [0, 3, 6, 7, 9, 10]\n",
      "topic ID: 477\tRun ID: despite-deer\tempty citation sentences: [8]\n",
      "topic ID: 477\tRun ID: satellite-gear\tempty citation sentences: [0, 1, 2, 5, 9, 12, 15]\n",
      "topic ID: 477\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 2, 4, 5, 6]\n",
      "topic ID: 477\tRun ID: plant-indeed\tempty citation sentences: [0, 4, 5]\n",
      "topic ID: 477\tRun ID: internal-withdraw\tempty citation sentences: [0, 3, 5, 7, 9, 11, 13, 15, 17]\n",
      "topic ID: 477\tRun ID: flag-square\tempty citation sentences: [9]\n",
      "topic ID: 477\tRun ID: previously-accept\tempty citation sentences: [14]\n",
      "topic ID: 477\tRun ID: peer-necessarily\tempty citation sentences: [0, 3, 5, 7, 9, 10, 13, 15]\n",
      "topic ID: 477\tRun ID: badly-salary\tempty citation sentences: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]\n",
      "topic ID: 477\tRun ID: listen-promise\tempty citation sentences: [13]\n",
      "topic ID: 499\tRun ID: satellite-gear\tempty citation sentences: [0, 2]\n",
      "topic ID: 499\tRun ID: arrange-final\tempty citation sentences: [0, 4, 8, 12, 15]\n",
      "topic ID: 499\tRun ID: flag-square\tempty citation sentences: [0, 8, 12]\n",
      "topic ID: 499\tRun ID: distinction-standard\tempty citation sentences: [19]\n",
      "topic ID: 499\tRun ID: worry-repeat\tempty citation sentences: [1]\n",
      "topic ID: 499\tRun ID: hear-arrangement\tempty citation sentences: [1]\n",
      "topic ID: 499\tRun ID: badly-salary\tempty citation sentences: [15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "topic ID: 499\tRun ID: eight-existence\tempty citation sentences: [8]\n",
      "topic ID: 499\tRun ID: review-basic\tempty citation sentences: [8]\n",
      "topic ID: 499\tRun ID: plant-indeed\tempty citation sentences: [0, 1, 5]\n",
      "topic ID: 499\tRun ID: internal-withdraw\tempty citation sentences: [0]\n",
      "topic ID: 499\tRun ID: peer-necessarily\tempty citation sentences: [0, 3]\n",
      "topic ID: 499\tRun ID: sport-broad\tempty citation sentences: [15]\n",
      "topic ID: 499\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 499\tRun ID: captain-wage\tempty citation sentences: [0, 8, 11]\n",
      "topic ID: 499\tRun ID: temperature-thought\tempty citation sentences: [0, 1, 2, 3, 5, 6, 7]\n",
      "topic ID: 499\tRun ID: listen-promise\tempty citation sentences: [16, 17]\n",
      "topic ID: 515\tRun ID: dirty-shop\tempty citation sentences: [5]\n",
      "topic ID: 515\tRun ID: despite-deer\tempty citation sentences: [1, 4, 7]\n",
      "topic ID: 515\tRun ID: satellite-gear\tempty citation sentences: [0, 5, 6]\n",
      "topic ID: 515\tRun ID: arrange-final\tempty citation sentences: [0, 5, 18, 19]\n",
      "topic ID: 515\tRun ID: flag-square\tempty citation sentences: [0, 4, 5]\n",
      "topic ID: 515\tRun ID: captain-wage\tempty citation sentences: [2, 5]\n",
      "topic ID: 515\tRun ID: internal-withdraw\tempty citation sentences: [7]\n",
      "topic ID: 515\tRun ID: peer-necessarily\tempty citation sentences: [0, 4]\n",
      "topic ID: 515\tRun ID: badly-salary\tempty citation sentences: [21, 22, 23, 24, 25]\n",
      "topic ID: 515\tRun ID: vary-occasion\tempty citation sentences: [7, 9]\n",
      "topic ID: 515\tRun ID: eight-existence\tempty citation sentences: [3, 4, 5]\n",
      "topic ID: 515\tRun ID: battery-founder\tempty citation sentences: [5, 7, 16]\n",
      "topic ID: 515\tRun ID: jacket-responsibility\tempty citation sentences: [5, 9, 14]\n",
      "topic ID: 515\tRun ID: plant-indeed\tempty citation sentences: [0]\n",
      "topic ID: 515\tRun ID: temperature-thought\tempty citation sentences: [0]\n",
      "topic ID: 707\tRun ID: dirty-shop\tempty citation sentences: [5, 9, 10]\n",
      "topic ID: 707\tRun ID: despite-deer\tempty citation sentences: [7, 8]\n",
      "topic ID: 707\tRun ID: satellite-gear\tempty citation sentences: [0]\n",
      "topic ID: 707\tRun ID: usual-disaster\tempty citation sentences: [4]\n",
      "topic ID: 707\tRun ID: battery-founder\tempty citation sentences: [7, 8]\n",
      "topic ID: 707\tRun ID: arrange-final\tempty citation sentences: [0, 6, 9, 12]\n",
      "topic ID: 707\tRun ID: flag-square\tempty citation sentences: [0, 9, 13, 17]\n",
      "topic ID: 707\tRun ID: captain-wage\tempty citation sentences: [1]\n",
      "topic ID: 707\tRun ID: image-hall\tempty citation sentences: [0, 1, 3, 4]\n",
      "topic ID: 707\tRun ID: badly-salary\tempty citation sentences: [17]\n",
      "topic ID: 707\tRun ID: review-basic\tempty citation sentences: [4]\n",
      "topic ID: 707\tRun ID: vary-occasion\tempty citation sentences: [1, 7]\n",
      "topic ID: 707\tRun ID: previously-accept\tempty citation sentences: [11]\n",
      "topic ID: 707\tRun ID: internal-withdraw\tempty citation sentences: [1, 5, 10]\n",
      "topic ID: 707\tRun ID: peer-necessarily\tempty citation sentences: [0, 2]\n",
      "topic ID: 707\tRun ID: sport-broad\tempty citation sentences: [0]\n",
      "topic ID: 707\tRun ID: concert-matter\tempty citation sentences: [0]\n",
      "topic ID: 707\tRun ID: degree-mental\tempty citation sentences: [4]\n",
      "topic ID: 707\tRun ID: jacket-responsibility\tempty citation sentences: [0, 1, 2, 8, 11]\n",
      "topic ID: 897\tRun ID: dirty-shop\tempty citation sentences: [0, 5]\n",
      "topic ID: 897\tRun ID: satellite-gear\tempty citation sentences: [0, 3]\n",
      "topic ID: 897\tRun ID: arrange-final\tempty citation sentences: [0]\n",
      "topic ID: 897\tRun ID: flag-square\tempty citation sentences: [0]\n",
      "topic ID: 897\tRun ID: image-hall\tempty citation sentences: [0]\n",
      "topic ID: 897\tRun ID: peer-necessarily\tempty citation sentences: [0]\n",
      "topic ID: 897\tRun ID: plant-indeed\tempty citation sentences: [0, 6]\n",
      "topic ID: 897\tRun ID: temperature-thought\tempty citation sentences: [0, 6]\n",
      "topic ID: 897\tRun ID: sport-broad\tempty citation sentences: [14]\n",
      "topic ID: 897\tRun ID: badly-salary\tempty citation sentences: [19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "topic ID: 897\tRun ID: concert-matter\tempty citation sentences: [13]\n"
     ]
    }
   ],
   "source": [
    "# load the empty predictions as empty citations and add them to the topicwise_results\n",
    "RUNFILE_DIR = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/\"\n",
    "\n",
    "for topic_id in topicwise_results:\n",
    "    run_ids = topicwise_results[topic_id]\n",
    "    for run_id in run_ids:\n",
    "        sentences_with_citations = set([sentence.get(\"sentenceID\") for sentence in run_ids[run_id][\"sentences\"]])\n",
    "        task = \"auggen\" if run_id in RUNFILES[\"auggen\"] else \"gen\"\n",
    "        results = load_runfile(os.path.join(RUNFILE_DIR, f\"{task}/{run_id}\"))\n",
    "        total_sentences = len(results[topic_id][\"answer\"])\n",
    "\n",
    "        if total_sentences > len(sentences_with_citations):\n",
    "            sentence_ids_missing = [idx for idx in range(0, total_sentences) if idx not in sentences_with_citations]\n",
    "            print(f\"topic ID: {topic_id}\\tRun ID: {run_id}\\tempty citation sentences: {sentence_ids_missing}\")\n",
    "\n",
    "            for idx in sentence_ids_missing:\n",
    "                topicwise_results[topic_id][run_id][\"sentences\"].append(\n",
    "                    {\n",
    "                        \"sentenceID\": idx,\n",
    "                        \"text\": results[topic_id][\"answer\"][idx]['text'],\n",
    "                        \"citations\": []\n",
    "                    }\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save these judgments to the output directory\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/results/support_assignment/human_judgments.jsonl\", \"w\") as f:\n",
    "    for topic_id, results in topicwise_results.items():\n",
    "        for run_id, value in results.items():\n",
    "            x = {\n",
    "                \"narrative_id\": topic_id,\n",
    "                \"run_id\": run_id,\n",
    "                \"sentences\": sorted(value[\"sentences\"], key=lambda x: x[\"sentenceID\"])\n",
    "            }\n",
    "            f.write(json.dumps(x) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total run IDs: 57\n"
     ]
    }
   ],
   "source": [
    "## Since we are only evaluating the top 3 runs from each team; lets find those runs\n",
    "\n",
    "ALL_RUN_IDS = set()\n",
    "anon_to_orig_map = {v: k for k, v in orig_to_anon_map.items()}\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/auggen-runs.top3.ordered\", \"r\") as f:\n",
    "    auggen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for original_run_id in auggen_run_tags:\n",
    "        anon_run_id = orig_to_anon_map.get(original_run_id, None)\n",
    "        if anon_run_id is not None:\n",
    "            ALL_RUN_IDS.add(anon_run_id)\n",
    "\n",
    "with open(\"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/runs/anon/tags/gen-runs.top3.ordered\", \"r\") as f:\n",
    "    gen_run_tags = [line.strip() for line in f.readlines()]\n",
    "    for original_run_id in gen_run_tags:\n",
    "        anon_run_id = orig_to_anon_map.get(original_run_id, None)\n",
    "        if anon_run_id is not None:\n",
    "            ALL_RUN_IDS.add(anon_run_id)\n",
    "\n",
    "print(f\"Total run IDs: {len(ALL_RUN_IDS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing topic ID: 14 with 75 runs\n",
      "Run ID: angle-apartment not there in topic ID: 14\n",
      "Processing topic ID: 72 with 74 runs\n",
      "Run ID: angle-apartment not there in topic ID: 72\n",
      "Run ID: attach-over not there in topic ID: 72\n",
      "Processing topic ID: 93 with 73 runs\n",
      "Run ID: angle-apartment not there in topic ID: 93\n",
      "Run ID: degree-mental not there in topic ID: 93\n",
      "Processing topic ID: 144 with 52 runs\n",
      "Run ID: bath-begin not there in topic ID: 144\n",
      "Run ID: flight-enhance not there in topic ID: 144\n",
      "Run ID: gently-disagree not there in topic ID: 144\n",
      "Run ID: transportation-error not there in topic ID: 144\n",
      "Run ID: worry-repeat not there in topic ID: 144\n",
      "Run ID: activity-manufacturer not there in topic ID: 144\n",
      "Processing topic ID: 161 with 51 runs\n",
      "Run ID: hear-arrangement not there in topic ID: 161\n",
      "Run ID: bath-begin not there in topic ID: 161\n",
      "Run ID: flight-enhance not there in topic ID: 161\n",
      "Run ID: gently-disagree not there in topic ID: 161\n",
      "Run ID: transportation-error not there in topic ID: 161\n",
      "Run ID: worry-repeat not there in topic ID: 161\n",
      "Run ID: activity-manufacturer not there in topic ID: 161\n",
      "Processing topic ID: 200 with 50 runs\n",
      "Run ID: bath-begin not there in topic ID: 200\n",
      "Run ID: neither-fitness not there in topic ID: 200\n",
      "Run ID: satellite-gear not there in topic ID: 200\n",
      "Run ID: flight-enhance not there in topic ID: 200\n",
      "Run ID: gently-disagree not there in topic ID: 200\n",
      "Run ID: transportation-error not there in topic ID: 200\n",
      "Run ID: activity-manufacturer not there in topic ID: 200\n",
      "Run ID: figure-civilian not there in topic ID: 200\n",
      "Processing topic ID: 213 with 52 runs\n",
      "Run ID: bath-begin not there in topic ID: 213\n",
      "Run ID: satellite-gear not there in topic ID: 213\n",
      "Run ID: flight-enhance not there in topic ID: 213\n",
      "Run ID: gently-disagree not there in topic ID: 213\n",
      "Run ID: transportation-error not there in topic ID: 213\n",
      "Run ID: activity-manufacturer not there in topic ID: 213\n",
      "Processing topic ID: 224 with 50 runs\n",
      "Run ID: bath-begin not there in topic ID: 224\n",
      "Run ID: neither-fitness not there in topic ID: 224\n",
      "Run ID: satellite-gear not there in topic ID: 224\n",
      "Run ID: flight-enhance not there in topic ID: 224\n",
      "Run ID: gently-disagree not there in topic ID: 224\n",
      "Run ID: transportation-error not there in topic ID: 224\n",
      "Run ID: activity-manufacturer not there in topic ID: 224\n",
      "Run ID: figure-civilian not there in topic ID: 224\n",
      "Processing topic ID: 233 with 51 runs\n",
      "Run ID: hear-arrangement not there in topic ID: 233\n",
      "Run ID: bath-begin not there in topic ID: 233\n",
      "Run ID: flight-enhance not there in topic ID: 233\n",
      "Run ID: gently-disagree not there in topic ID: 233\n",
      "Run ID: transportation-error not there in topic ID: 233\n",
      "Run ID: worry-repeat not there in topic ID: 233\n",
      "Run ID: activity-manufacturer not there in topic ID: 233\n",
      "Processing topic ID: 273 with 75 runs\n",
      "Run ID: angle-apartment not there in topic ID: 273\n",
      "Processing topic ID: 300 with 51 runs\n",
      "Run ID: bath-begin not there in topic ID: 300\n",
      "Run ID: neither-fitness not there in topic ID: 300\n",
      "Run ID: flight-enhance not there in topic ID: 300\n",
      "Run ID: gently-disagree not there in topic ID: 300\n",
      "Run ID: transportation-error not there in topic ID: 300\n",
      "Run ID: activity-manufacturer not there in topic ID: 300\n",
      "Run ID: figure-civilian not there in topic ID: 300\n",
      "Processing topic ID: 407 with 75 runs\n",
      "Run ID: angle-apartment not there in topic ID: 407\n",
      "Processing topic ID: 477 with 51 runs\n",
      "Run ID: hear-arrangement not there in topic ID: 477\n",
      "Run ID: bath-begin not there in topic ID: 477\n",
      "Run ID: flight-enhance not there in topic ID: 477\n",
      "Run ID: gently-disagree not there in topic ID: 477\n",
      "Run ID: transportation-error not there in topic ID: 477\n",
      "Run ID: worry-repeat not there in topic ID: 477\n",
      "Run ID: activity-manufacturer not there in topic ID: 477\n",
      "Processing topic ID: 499 with 53 runs\n",
      "Run ID: bath-begin not there in topic ID: 499\n",
      "Run ID: flight-enhance not there in topic ID: 499\n",
      "Run ID: gently-disagree not there in topic ID: 499\n",
      "Run ID: transportation-error not there in topic ID: 499\n",
      "Run ID: activity-manufacturer not there in topic ID: 499\n",
      "Processing topic ID: 515 with 53 runs\n",
      "Run ID: bath-begin not there in topic ID: 515\n",
      "Run ID: flight-enhance not there in topic ID: 515\n",
      "Run ID: gently-disagree not there in topic ID: 515\n",
      "Run ID: transportation-error not there in topic ID: 515\n",
      "Run ID: activity-manufacturer not there in topic ID: 515\n",
      "Processing topic ID: 707 with 53 runs\n",
      "Run ID: bath-begin not there in topic ID: 707\n",
      "Run ID: flight-enhance not there in topic ID: 707\n",
      "Run ID: gently-disagree not there in topic ID: 707\n",
      "Run ID: transportation-error not there in topic ID: 707\n",
      "Run ID: activity-manufacturer not there in topic ID: 707\n",
      "Processing topic ID: 897 with 53 runs\n",
      "Run ID: bath-begin not there in topic ID: 897\n",
      "Run ID: flight-enhance not there in topic ID: 897\n",
      "Run ID: gently-disagree not there in topic ID: 897\n",
      "Run ID: transportation-error not there in topic ID: 897\n",
      "Run ID: activity-manufacturer not there in topic ID: 897\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "weighted_scores = {-1: 0, 0: 0, 1: 0.5, 2: 1}\n",
    "hard_scores = {-1: 0, 0: 0, 1: 0, 2: 1}\n",
    "\n",
    "# Create a directory to store the results\n",
    "OUTPUT_DIR = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/results/support_assignment/leaderboard/participant_results/human/\"\n",
    "auggen_output_dir = os.path.join(OUTPUT_DIR, \"auggen\")\n",
    "gen_output_dir = os.path.join(OUTPUT_DIR, \"gen\")\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(auggen_output_dir, exist_ok=True)\n",
    "os.makedirs(gen_output_dir, exist_ok=True)\n",
    "\n",
    "# intiliaze the scores for each run ID\n",
    "all_run_scores = {run_id: {} for run_id in ALL_RUN_IDS}\n",
    "\n",
    "for topic_id in topicwise_results:\n",
    "    individual_results = topicwise_results.get(topic_id, {})\n",
    "    print(f\"Processing topic ID: {topic_id} with {len(individual_results)} runs\")\n",
    "\n",
    "    # Iterate over each run ID, sentences\n",
    "    for run_id, value in individual_results.items():\n",
    "        \n",
    "        if run_id not in ALL_RUN_IDS:\n",
    "            continue\n",
    "        \n",
    "        sentences = value.get(\"sentences\", [])\n",
    "        sent_with_citations, weighted_score, hard_score = 0, 0.0, 0.0\n",
    "        total_count_sentences = len(sentences)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if len(sentence.get('citations')) > 0:\n",
    "                first_citation = sentence.get('citations')[0]\n",
    "                support_score = int(first_citation.get('support'))\n",
    "                \n",
    "                if support_score > -1:\n",
    "                    # numerator\n",
    "                    weighted_score += weighted_scores.get(support_score)\n",
    "                    hard_score += hard_scores.get(support_score)\n",
    "                    sent_with_citations += 1\n",
    "                \n",
    "                elif support_score == -1:\n",
    "                    total_count_sentences -= 1\n",
    "        \n",
    "         # Write the results to a file\n",
    "        output_dir = auggen_output_dir if run_id in RUNFILES[\"auggen\"] else gen_output_dir\n",
    "        original_run_id = anon_to_orig_map.get(run_id)\n",
    "        with open(f\"{output_dir}/{original_run_id}.jsonl\", \"a\") as f:\n",
    "            values = {\n",
    "                \"topic_id\": topic_id,\n",
    "                \"weighted_precision\": weighted_score / sent_with_citations if sent_with_citations > 0 else 0,\n",
    "                \"hard_precision\": hard_score / sent_with_citations if sent_with_citations > 0 else 0,\n",
    "                \"weighted_recall\": weighted_score / total_count_sentences if total_count_sentences > 0 else 0,\n",
    "                \"hard_recall\": hard_score / total_count_sentences if total_count_sentences > 0 else 0,\n",
    "                \"sentences\": len(sentences)\n",
    "            }\n",
    "            f.write(json.dumps(values) + \"\\n\")\n",
    "    \n",
    "    ## add the missing run IDs for the topic ID\n",
    "    for run_id_temp in ALL_RUN_IDS:\n",
    "        if run_id_temp not in individual_results:\n",
    "            print(f\"Run ID: {run_id_temp} not there in topic ID: {topic_id}\")\n",
    "            output_dir = auggen_output_dir if run_id_temp in RUNFILES[\"auggen\"] else gen_output_dir\n",
    "            original_run_id = anon_to_orig_map.get(run_id_temp)\n",
    "            with open(f\"{output_dir}/{original_run_id}.jsonl\", \"a\") as f:\n",
    "                values = {\n",
    "                    \"topic_id\": topic_id,\n",
    "                    \"weighted_precision\": 0.0,\n",
    "                    \"hard_precision\": 0.0,\n",
    "                    \"weighted_recall\": 0.0,\n",
    "                    \"hard_recall\": 0.0,\n",
    "                    \"sentences\": 0.0\n",
    "                }\n",
    "                f.write(json.dumps(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPUTE AVERAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the average scores for each run ID in the merged file directory\n",
    "import glob\n",
    "\n",
    "total_topics = 17\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"auggen\": auggen_output_dir,\n",
    "    \"gen\": gen_output_dir\n",
    "}\n",
    "\n",
    "for run_id in ALL_RUN_IDS:\n",
    "    merged_results = []\n",
    "    task = \"auggen\" if run_id in RUNFILES[\"auggen\"] else \"gen\"\n",
    "    output_dir = OUTPUT_DIRS[task]\n",
    "    original_run_id = anon_to_orig_map.get(run_id)\n",
    "    for file in glob.glob(f\"{output_dir}/{original_run_id}.jsonl\"):\n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                merged_results.append(json.loads(line))\n",
    "    \n",
    "    with open(f\"{output_dir}/{original_run_id}.jsonl\", \"a\") as f:\n",
    "        values = {\n",
    "            \"topic_id\": \"all\",\n",
    "            \"weighted_precision\": sum([v[\"weighted_precision\"] for v in merged_results]) / total_topics,\n",
    "            \"hard_precision\": sum([v[\"hard_precision\"] for v in merged_results]) / total_topics,\n",
    "            \"weighted_recall\": sum([v[\"weighted_recall\"] for v in merged_results]) / total_topics,\n",
    "            \"hard_recall\": sum([v[\"hard_recall\"] for v in merged_results]) / total_topics,\n",
    "            \"sentences\": sum([v[\"sentences\"] for v in merged_results]) / total_topics,\n",
    "            \"topics_evaluated\": total_topics - sum(1 for v in merged_results if v[\"sentences\"] == 0),\n",
    "        }\n",
    "        f.write(json.dumps(values) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing run ID: ag-v2-gpt\n",
      "Processing run ID: wingii-3-rl-refined\n",
      "Processing run ID: ag-v1-gpt\n",
      "Processing run ID: no-llm-refined\n",
      "Processing run ID: ag-v2-llama\n",
      "Processing run ID: wingii-v3-gpt\n",
      "Processing run ID: gpt41\n",
      "Processing run ID: IDACCS-nugg-gpt-4-1\n",
      "Processing run ID: Rerank-Top50_v3\n",
      "Processing run ID: IDACCSabstrct-gpt4-1\n",
      "Processing run ID: IDACCS-hybrid-gpt4-1\n",
      "Processing run ID: Rerank-Top50_v2\n",
      "Processing run ID: cluster-generation\n",
      "Processing run ID: nugget-generation\n",
      "Processing run ID: cru-ablR\n",
      "Processing run ID: cru-ansR-conf\n",
      "Processing run ID: rag25_qwen3_50_ag\n",
      "Processing run ID: rag25_qwen3_20_ag\n",
      "Processing run ID: cru-ansR\n",
      "Processing run ID: NITA_AG_JH\n",
      "Processing run ID: uema2lab_B4\n",
      "Processing run ID: hltcoe-gpt5.searcher\n",
      "Processing run ID: r_4method_ag_gpt41\n",
      "Processing run ID: hltcoe-lg.qwen\n",
      "Processing run ID: KG-AG-1\n",
      "Processing run ID: hltcoe-lg.searcher\n",
      "Processing run ID: lg_nt_q4d12l3_c\n",
      "Processing run ID: bm25-rz7b-2025a\n",
      "Processing run ID: no-reranker\n",
      "Processing run ID: full\n",
      "Processing run ID: gptr.nt_q4d4\n",
      "Processing run ID: grilllab-agentic-gpt4-generation\n",
      "Processing run ID: rag-v2-gpt\n",
      "Processing run ID: r_2method_ag_gpt41\n",
      "Processing run ID: rag-v1-gpt\n",
      "Processing run ID: gptr_e2_q4d4\n",
      "Processing run ID: selector-agent-trim\n",
      "Processing run ID: LAS-agentic-RAG-selector\n",
      "Processing run ID: rag25_test_qwen3_50\n",
      "Processing run ID: rag25_test_qwen3_20\n",
      "Processing run ID: no-decomp\n",
      "Processing run ID: auto_selected\n",
      "Processing run ID: grilllab-gpt45-gen\n",
      "Processing run ID: auto_plan\n",
      "Processing run ID: combined\n",
      "Processing run ID: e5_monot5_searchR1\n",
      "Processing run ID: rag-v2-llama\n",
      "Processing run ID: strd_roll_segment\n",
      "Processing run ID: standard_roll\n",
      "Processing run ID: LAS-agentic-RAG-agent\n",
      "Processing run ID: sub_query_entities\n",
      "Processing run ID: ori_query_entities\n",
      "Processing run ID: ag-run-1-JH\n",
      "Processing run ID: genSubQ_merge\n",
      "Processing run ID: uema2lab_base\n",
      "Processing run ID: Kun-Third\n",
      "Processing run ID: uema2lab_rag_fewdoc\n"
     ]
    }
   ],
   "source": [
    "# print the final scores for each run ID as a csv file\n",
    "import pandas as pd\n",
    "\n",
    "for task in [\"auggen\", \"gen\"]:\n",
    "    output_dir = OUTPUT_DIRS[task]\n",
    "    all_results = []\n",
    "    for file in glob.glob(f\"{output_dir}/*.jsonl\"):\n",
    "        run_id = os.path.basename(file).replace(\".jsonl\", \"\")\n",
    "        print(f\"Processing run ID: {run_id}\")\n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                json_line = json.loads(line)\n",
    "\n",
    "                if json_line[\"topic_id\"] == \"all\":\n",
    "                    all_results.append({\n",
    "                        \"run_id\": run_id, \n",
    "                        \"Weighted Precision\": round(json_line[\"weighted_precision\"], 3),\n",
    "                        \"Hard Precision\": round(json_line[\"hard_precision\"], 3),\n",
    "                        \"Weighted Recall\": round(json_line[\"weighted_recall\"], 3),\n",
    "                        \"Hard Recall\": round(json_line[\"hard_recall\"], 3),\n",
    "                        \"Sentences\": round(json_line[\"sentences\"], 3),\n",
    "                        \"Topics Evaluated\": json_line[\"topics_evaluated\"],\n",
    "                    })\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.sort_values(by=[\"Weighted Precision\"], ascending=False)\n",
    "    df.to_csv(f\"{output_dir}/../{task}_support_results_17_topics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge per-topic runs across participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = [\n",
    "    \"14\",\"72\",\"93\",\"144\",\"161\",\"200\",\"213\",\"224\",\"233\",\"273\",\"300\",\"407\",\"477\",\"499\",\"515\",\"707\",\"897\",\"all\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total topics in merged results: 18\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "OUTPUT_DIR = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/results/support_assignment/leaderboard/participant_results/human/\"\n",
    "\n",
    "# Create a directory to store the results\n",
    "topic_output_dir = os.path.join(OUTPUT_DIR, \"per-topic\")\n",
    "\n",
    "OUTPUT_DIRS = {\n",
    "    \"auggen\": topic_output_dir,\n",
    "    \"gen\": topic_output_dir\n",
    "}\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(topic_output_dir, exist_ok=True)\n",
    "\n",
    "# merge the jsonl files for each directory\n",
    "import glob, os, json\n",
    "\n",
    "merged_results = {topic_id: {} for topic_id in topic_ids}\n",
    "\n",
    "for run_id in ALL_RUN_IDS:\n",
    "    task = \"auggen\" if run_id in RUNFILES[\"auggen\"] else \"gen\"\n",
    "    original_run_id = anon_to_orig_map.get(run_id)\n",
    "    for file in glob.glob(f\"{OUTPUT_DIR}/{task}/{original_run_id}.jsonl\"): \n",
    "        with open(file, \"r\") as f:\n",
    "            for line in f:\n",
    "                topic_id = json.loads(line)[\"topic_id\"]\n",
    "                merged_results[topic_id][original_run_id] = json.loads(line)\n",
    "\n",
    "print(f\"Total topics in merged results: {len(merged_results)}\")\n",
    "\n",
    "# write the results to the output directory\n",
    "with open(f\"{topic_output_dir}/mean_topicwise_scores.jsonl\", \"w\") as f:\n",
    "    for topic_id in topic_ids:\n",
    "        # runfiles for each topic\n",
    "        runfiles = merged_results[topic_id]\n",
    "        ## compute average scores across all run_ids\n",
    "        total_runfiles = len(runfiles)\n",
    "        \n",
    "        values = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"weighted_precision\": sum([v[\"weighted_precision\"] for v in runfiles.values()]) / total_runfiles,\n",
    "            \"hard_precision\": sum([v[\"hard_precision\"] for v in runfiles.values()]) / total_runfiles,\n",
    "            \"weighted_recall\": sum([v[\"weighted_recall\"] for v in runfiles.values()]) / total_runfiles,\n",
    "            \"hard_recall\": sum([v[\"hard_recall\"] for v in runfiles.values()]) / total_runfiles,\n",
    "            \"sentences\": sum([v[\"sentences\"] for v in runfiles.values()]) / total_runfiles\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(values) + \"\\n\")\n",
    "\n",
    "# write the results to the output directory\n",
    "with open(f\"{topic_output_dir}/median_topicwise_scores.jsonl\", \"w\") as f:\n",
    "    for topic_id in topic_ids:\n",
    "        # runfiles for each topic\n",
    "        runfiles = merged_results[topic_id]\n",
    "        ## compute average scores across all run_ids\n",
    "        total_runfiles = len(runfiles)\n",
    "        \n",
    "        # get the scores for each run_id\n",
    "        hard_precision = np.array([v[\"hard_precision\"] for v in runfiles.values()])\n",
    "        weighted_precision = np.array([v[\"weighted_precision\"] for v in runfiles.values()])\n",
    "        weighted_recall = np.array([v[\"weighted_recall\"] for v in runfiles.values()])\n",
    "        hard_recall = np.array([v[\"hard_recall\"] for v in runfiles.values()])\n",
    "        sentences = np.array([v[\"sentences\"] for v in runfiles.values()])\n",
    "\n",
    "        values = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"weighted_precision\": float(np.median(weighted_precision)),\n",
    "            \"hard_precision\": float(np.median(hard_precision)),\n",
    "            \"weighted_recall\": float(np.median(weighted_recall)),\n",
    "            \"hard_recall\": float(np.median(hard_recall)),\n",
    "            \"sentences\": float(np.median(sentences))\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(values) + \"\\n\")\n",
    "\n",
    "# write the results to the output directory\n",
    "with open(f\"{topic_output_dir}/max_topicwise_scores.jsonl\", \"w\") as f:\n",
    "    for topic_id in topic_ids:\n",
    "        # runfiles for each topic\n",
    "        runfiles = merged_results[topic_id]\n",
    "        ## compute average scores across all run_ids\n",
    "        total_runfiles = len(runfiles)\n",
    "        \n",
    "        # get the scores for each run_id\n",
    "        hard_precision = np.array([v[\"hard_precision\"] for v in runfiles.values()])\n",
    "        weighted_precision = np.array([v[\"weighted_precision\"] for v in runfiles.values()])\n",
    "        weighted_recall = np.array([v[\"weighted_recall\"] for v in runfiles.values()])\n",
    "        hard_recall = np.array([v[\"hard_recall\"] for v in runfiles.values()])\n",
    "        sentences = np.array([v[\"sentences\"] for v in runfiles.values()])\n",
    "\n",
    "        values = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"weighted_precision\": float(np.max(weighted_precision)),\n",
    "            \"hard_precision\": float(np.max(hard_precision)),\n",
    "            \"weighted_recall\": float(np.max(weighted_recall)),\n",
    "            \"hard_recall\": float(np.max(hard_recall)),\n",
    "            \"sentences\": float(np.max(sentences))\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(values) + \"\\n\")\n",
    "\n",
    "# write the results to the output directory\n",
    "with open(f\"{topic_output_dir}/min_topicwise_scores.jsonl\", \"w\") as f:\n",
    "    for topic_id in topic_ids:\n",
    "        # runfiles for each topic\n",
    "        runfiles = merged_results[topic_id]\n",
    "        ## compute average scores across all run_ids\n",
    "        total_runfiles = len(runfiles)\n",
    "        \n",
    "        # get the scores for each run_id\n",
    "        hard_precision = np.array([v[\"hard_precision\"] for v in runfiles.values()])\n",
    "        weighted_precision = np.array([v[\"weighted_precision\"] for v in runfiles.values()])\n",
    "        weighted_recall = np.array([v[\"weighted_recall\"] for v in runfiles.values()])\n",
    "        hard_recall = np.array([v[\"hard_recall\"] for v in runfiles.values()])\n",
    "        sentences = np.array([v[\"sentences\"] for v in runfiles.values()])\n",
    "\n",
    "        values = {\n",
    "            \"topic_id\": topic_id,\n",
    "            \"weighted_precision\": float(np.min(weighted_precision)),\n",
    "            \"hard_precision\": float(np.min(hard_precision)),\n",
    "            \"weighted_recall\": float(np.min(weighted_recall)),\n",
    "            \"hard_recall\": float(np.min(hard_recall)),\n",
    "            \"sentences\": float(np.min(sentences))\n",
    "        }\n",
    "        \n",
    "        f.write(json.dumps(values) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIRS = {\n",
    "    \"auggen\": auggen_output_dir,\n",
    "    \"gen\": gen_output_dir\n",
    "}\n",
    "\n",
    "OUTPUT_FINAL_DIR = \"/store/scratch/n3thakur/trec-rag-2024/trec2024-rag/support_eval/2025/trec25-rag/results/final_dump/\"\n",
    "\n",
    "for task in [\"auggen\", \"gen\"]:\n",
    "    with open(f\"{OUTPUT_FINAL_DIR}/{task}_support_nist_w_topics.txt\", \"w\") as fout:\n",
    "        output_dir = OUTPUT_DIRS[task]\n",
    "        all_results = []\n",
    "        anonymous_run_ids = [orig_to_anon_map.get(os.path.basename(file).replace(\".jsonl\", \"\")) for file in glob.glob(f\"{output_dir}/*.jsonl\")]\n",
    "\n",
    "        for run_id in sorted(anonymous_run_ids):\n",
    "            original_run_id = anon_to_orig_map.get(run_id)\n",
    "            for metric in [\"weighted_precision\", \"weighted_recall\"]:\n",
    "                with open(f\"{output_dir}/{original_run_id}.jsonl\", \"r\") as f:\n",
    "                    for line in f:\n",
    "                        json_line = json.loads(line)\n",
    "                        if json_line[\"topic_id\"] != \"all\":\n",
    "                            topic_id = json_line[\"topic_id\"]\n",
    "                            if round(json_line[metric], 3) > 0.00001:\n",
    "                                fout.write(f\"{run_id} {topic_id} {metric} {round(json_line[metric], 3)}\")\n",
    "                                fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
